# Building Local-First Pipelines for Indic Knowledge Preservation
## YouTube Transcription & Verbatim OCR with Gemini (Research Project)

Over the past few weeks, a group of like-minded practitioners and engineers have been exploring a question that is often under-addressed:

**How can long-form Indic knowledge‚Äîspoken and written‚Äîbe digitised accurately, without losing linguistic fidelity or altering the original meaning?**

This exploration resulted in two focused, local-first research pipelines, built and tested on a local macOS setup:

1. YouTube ‚Üí Hindi transcription  
2. Scanned Jain scripture PDFs ‚Üí verbatim Hindi / Sanskrit / Prakrit text  

Both pipelines are guided by the same principles:

- deterministic and auditable outputs  
- strict prompt control  
- zero interpretation or normalization  
- deep respect for original source material  

---

## Capability 1: Transcribing YouTube Videos to Hindi

### Problem

A significant amount of Jain pravachans and discourses exists only as long YouTube videos:

- captions are often missing or unreliable  
- auto-generated subtitles struggle with religious vocabulary  
- honorifics, names, and context are frequently distorted  

Manual transcription is time-consuming and difficult to scale.

### What Was Built

A simple, research-oriented transcription pipeline that:

- accepts YouTube video URLs only, keeping the workflow intentionally simple  
- downloads audio-only (MP3) using `yt-dlp`  
- caches audio and metadata to avoid repeated processing  
- performs prompt-controlled Gemini transcription into Hindi  
- heuristically extracts speaker names from video titles  
- generates clean, UTF-8 encoded Hindi text files  
- archives outputs from previous runs for traceability  

Transcriptions are generated strictly for analysis and preservation purposes.

---

## Capability 2: Verbatim OCR for Jain Scriptures

### Problem

Many Jain scriptures exist today only as scanned PDFs that are:

- decades or even centuries old  
- printed using older presses and fonts  
- affected by degraded scanned quality for documents printed decades ago and digitised much later  
- in some cases, difficult to read even for humans  

Often, the original print quality was already weak, and the eventual scanning process further reduced legibility. Some pages are faded, skewed, or partially unreadable.

Traditional OCR systems tend to:

- normalize text  
- ‚Äúcorrect‚Äù characters  
- drop uncommon glyphs  
- lose line and paragraph structure  

For archival and scholarly work, this behaviour is unacceptable.

### What Was Built

A page-accurate, verbatim OCR pipeline using Gemini via Vertex AI that:

- converts PDFs into high-DPI page images  
- processes one page at a time  
- enforces a strict archivist-style prompt  
- preserves:  
  - exact Devanagari glyphs  
  - matras, conjuncts, punctuation  
  - special symbols (‡•ê, Âçê, ‡••, ‡•§)  
  - original line breaks and spacing  
- caches OCR output page-by-page  
- deterministically rebuilds a final text file  

The system does **not** translate, summarize, interpret, or ‚Äúfix‚Äù text.

If a character is unclear in the scanned page, it remains unclear in the output.  
This constraint is deliberate and essential for preservation work.

---

## Example: Scanned Page ‚Üí Generated Verbatim Text

Below is a scanned page from a Jain text (as shown above), followed by the exact verbatim text generated by the OCR pipeline.

```
‡§¨‡§æ‡§∞‡§π ‡§≠‡§æ‡§µ‡§®‡§æ

( ‡§≠‡•Ç‡§ß‡§ó‡•ç‡§¶‡§æ‡§Æ )

( ‡•ß )
‡§∞‡§æ‡§ú‡§æ ‡§ó‡§£‡§æ ‡§§‡•ç‡§∞‡§™‡§®‡§ø, ‡§π‡§æ‡§•‡§ø‡§® ‡§ï‡•á ‡§Ö‡§∏‡§µ‡§æ‡§∞ ‡•§
‡§Æ‡§ó‡•ç‡§®‡§æ ‡§∏‡§¨‡§ï‡§æ ‡§è‡§ï ‡§¶‡§ø‡§®, ‡§Ö‡§™‡§®‡•Ä-‡§Ö‡§™‡§®‡•Ä ‡§¨‡§æ‡§∞ ‡•§‡•§

( ‡•® )
‡§¶‡§®‡•ç‡§®‡§µ‡§≤ ‡§¶‡•á‡§à ‡§¶‡•á‡§ñ‡§®‡§æ, ‡§Æ‡§æ‡§§-‡§™‡§ø‡§§‡§æ ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡•§
‡§Æ‡§∞‡§®‡•Ä ‡§µ‡§ø‡§ó‡•ç‡§Ø‡§æ ‡§ú‡•Ä‡§µ ‡§ï‡•ã, ‡§ï‡•ã‡§à ‡§® ‡§∞‡§æ‡§ñ‡§®‡§π‡§æ‡§∞ ‡•§‡•§

( ‡•© )
‡§¶‡§æ‡§Æ ‡§¨‡§ø‡§®‡§æ ‡§®‡§ø‡§∞‡•ç‡§ß‡§® ‡§¶‡•Å‡§ñ‡•Ä, ‡§§‡•É‡§∑‡•ç‡§£‡§æ‡§µ‡§∂ ‡§ß‡§®‡§µ‡§æ‡§® ‡•§
‡§ï‡§∑‡•ç‡§ü ‡§® ‡§∏‡•Å‡§ñ ‡§∏‡§Ç‡§∏‡§æ‡§∞ ‡§Æ‡•á‡§Ç, ‡§∏‡§¨ ‡§ú‡§ó ‡§¶‡•á‡§ñ‡•ç‡§Ø‡•ã ‡§¶‡§æ‡§® ‡•§‡•§

( ‡•™ )
‡§Ü‡§™ ‡§Ö‡§ï‡•á‡§≤‡•ã ‡§Ö‡§µ‡§§‡§∞‡•á, ‡§Æ‡§∞‡•á ‡§Ö‡§ï‡•á‡§≤‡•ã ‡§π‡•ã‡§Ø ‡•§
‡§Ø ‡§ï‡§¨‡§π‡•Ç ‡§á‡§∏ ‡§ú‡•Ä‡§µ ‡§ï‡§æ, ‡§∏‡§æ‡§•‡•Ä ‡§∏‡§ó‡§æ ‡§® ‡§ï‡•ã‡§Ø ‡•§‡•§

( ‡•´ )
‡§ú‡§π‡§æ ‡§¶‡•á‡§π ‡§Ö‡§™‡§®‡•Ä ‡§®‡§π‡•Ä‡§Ç, ‡§§‡§π‡§æ ‡§® ‡§Ö‡§™‡§®‡•ã ‡§ï‡§æ‡§Ø ‡•§
‡§ò‡§∞ ‡§∏‡§Ç‡§™‡§§‡•ç‡§§‡§ø ‡§™‡§∞ ‡§™‡•ç‡§∞‡§ó‡§ü ‡§Ø‡•á, ‡§™‡§∞ ‡§π‡•à‡§Ç ‡§™‡§∞‡§ø‡§ú‡§® ‡§≤‡•ã‡§Ø ‡•§‡•§

( ‡•¨ )
‡§¶‡§ø‡§™‡•à ‡§ö‡§æ‡§Æ ‡§ö‡§æ‡§¶‡§∞ ‡§Æ‡§¢‡§º‡•Ä, ‡§π‡§æ‡§°‡§º ‡§™‡•Ä‡§ú‡§ó ‡§¶‡•á‡§π ‡•§
‡§≠‡•Ä‡§§‡§∞ ‡§Ø‡§æ ‡§∏‡§Æ ‡§ú‡§ó‡§§ ‡§Æ‡•á‡§Ç, ‡§Ö‡§¨‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§ø‡§® ‡§ó‡•á‡§π ‡•§‡•§
```

**No corrections. No normalization. No interpretation.**  
Only what is visibly present in the scanned document.

---

## Mental Model Behind the System (High Level)

At a conceptual level, the system follows a layered mental model:

```
User
 ‚Üì
Domain-aware chat interface (rules, tone, constraints)
 ‚Üì
Planning and decision layer (what to do, when to retry, when to stop)
 ‚Üì
Specialised tools (YouTube transcription, OCR, search, storage)
 ‚Üì
Curated knowledge base (digitised Jain texts and transcripts)
```

This separation of intent, reasoning, execution, and knowledge allows each layer to evolve independently.  
Further details on how this model evolves will be shared in a future article.

---

## Architecture & Tooling (Local-First)

- Python (single-purpose scripts, minimal abstraction)  
- yt-dlp, ffmpeg for media handling  
- pdf2image, PIL for document processing  
- Gemini 2.5 Flash with strict prompt control  
- Vertex AI for stable OCR inference  
- UTF-8-safe logging throughout  
- GitHub as a controlled document source  

All experimentation and testing were performed locally.

---

## Open-Source & Reproducibility

The complete implementation is openly available on GitHub:  
üëâ https://github.com/arpit-jain-mygit/gemini-video-transcription-and-ocr-texts

Anyone interested is welcome to:

- review the architecture  
- run the pipelines locally  
- adapt them for their own research or archival use  

---

## Responsible and Legal Use of AI

This work is intended strictly for research, analysis, and preservation.

- Only publicly accessible YouTube videos were processed  
- No commercial use is intended  
- OCR was performed only on documents with appropriate access rights  

If you are working on similar systems, it is important to respect:

- platform terms of service  
- creator and author rights  
- applicable local laws and regulations  

---

## Acknowledgements & Closing Thoughts

This work has been shaped through discussions with like-minded people who care deeply about:

- Indic knowledge preservation  
- responsible application of Generative AI  
- building systems that respect original sources  

I would like to acknowledge and tag a few such individuals whose perspectives, technical rigor, and curiosity have influenced this work:

- Sunny Jain ‚Äì https://www.linkedin.com/in/sunny-jain-7482032b/  
- Pankaj Jain (Qualcomm, Hyderabad)  
- Atishay Jain ‚Äì https://www.linkedin.com/in/jainatishay71/  
- Divyanshi Jain ‚Äì https://www.linkedin.com/in/divyanshi342/  
- Kapil Jain ‚Äì https://www.linkedin.com/in/kapil-jain-918ab58/  

If you are working in related areas‚Äîtranscription pipelines, OCR for Indic scripts, archival digitisation, or responsible AI system design‚Äîit would be great to connect and exchange ideas.
